{
    "nbformat": 4, 
    "metadata": {
        "language_info": {
            "name": "python", 
            "nbconvert_exporter": "python", 
            "mimetype": "text/x-python", 
            "version": "2.7.11", 
            "codemirror_mode": {
                "name": "ipython", 
                "version": 2
            }, 
            "file_extension": ".py", 
            "pygments_lexer": "ipython2"
        }, 
        "kernelspec": {
            "name": "python2-spark20", 
            "display_name": "Python 2 with Spark 2.0", 
            "language": "python"
        }
    }, 
    "nbformat_minor": 0, 
    "cells": [
        {
            "metadata": {}, 
            "source": "<table style=\"border: none\" align=\"left\">\n<tr style=\"border: none\">\n<th style=\"border: none\"><img src=\"http://i.imgur.com/o1X3CAd.jpg\" alt=\"Icon\" align=\"left\"></th>\n</tr>\n   <tr style=\"border: none\">\n      <th style=\"border: none\"><font face=\"verdana\" size=\"6\" color=\"black\"><b>Cognihack Data Science Track</b></font></th>\n   </tr>\n   <tr style=\"border: none\">\n      <th style=\"border: none\"><font face=\"verdana\" size=\"5\" color=\"black\"><b>Predicting repeat shopping likelihood with Python, Spark and Watson Machine Learning</b></font></th>\n   </tr>\n   <tr style=\"border: none\">\n       <th style=\"border: none\"><img src=\"https://kaggle2.blob.core.windows.net/competitions/kaggle/3897/media/shoppers_lores.png\" alt=\"Icon\" align=\"left\"> </th>\n   </tr>\n</table>", 
            "cell_type": "markdown"
        }, 
        {
            "metadata": {}, 
            "source": "This notebook will take you through the process of creating a predictive model in Python using the data manipulation and machine learning libraries distributed with Spark.  \n- The data we are using here is an open source dataset prepared for a [machine learning challenge](https://www.kaggle.com/c/acquire-valued-shoppers-challenge) hosted on the [Kaggle](https://www.kaggle.com/) website.  \n\nOnce we've worked through the process of reading, understanding and preparing our data and have built a simple model together, we'll deploy it to the [Watson Machine Learning service](https://datascience.ibm.com/features#machinelearning) and make it available as a real-time scoring service.  \n\nYou should spend the remaining time working as a group to speculate on how you might improve these predictions. The cognihack tutors will endeavour to assist with any experimentation to help you create and evaluate refinements to the baseline model.\n\n# Learning goals\n\nThe learning goals of this exercise are:\n\n-  Loading CSV files into an Apache\u00ae Spark DataFrame.\n-  Exploring the data using the features within:  \n    a) Spark's data wrangling Python API: __pyspark.sql__;  \n    b) the __pandas__ data wrangling library; and  \n    c) __matplotlib__ for exploratory plots.  \n-  Engineering some basic predictive features, again using __pyspark.sql__ and Spark __user defined functions (UDFs)__.\n-  Preparing the data for training and evaluation.\n-  Creating an Apache\u00ae Spark machine learning pipeline.\n-  Training and evaluating a model.\n-  Persisting a pipeline and model in Watson Machine Learning repository.\n-  Deploying the model for online scoring using Wastson Machine Learning API.\n-  Scoring sample scoring data using the Watson Machine Learning API.\n\n\n# Contents\n\nThis notebook contains the following parts:\n\n1.\t[Setup](#setup)\n2.\t[Load and understand data](#load)\n3.  [Prepare dataset](#prepare)\n4.\t[Create a basic model](#model)\n5.\t[Deploy and score](#deploy)\n6.\t[Taking it further](#tuning)", 
            "cell_type": "markdown"
        }, 
        {
            "metadata": {}, 
            "source": "<a id=\"setup\"></a>\n# 1. Setup\nBefore we begin working through this notebook, you must perform the following setup tasks:\n\n-  Sign up for the IBM Data Science Experience (using w3 credentials) and create a new project;\n-  Retrieve the **Acquire Valued Shoppers Challenge** data from the [Cognihack Box folder](https://ibm.ent.box.com/folder/25592156534);\n-  Import the data (in csv format) into the Data Science Experience as a 'data asset' (the transactions_subset.csv file will require decompression prior to upload);\n-  Make sure that you are using a Spark 2.0 kernel and Python 2.x; and\n-  Create a [Watson Machine Learning Service](https://console.ng.bluemix.net/catalog/services/ibm-watson-machine-learning/) instance (a free plan is offered).", 
            "cell_type": "markdown"
        }, 
        {
            "metadata": {}, 
            "source": "<a id=\"load\"></a>\n# 2. Load and explore data", 
            "cell_type": "markdown"
        }, 
        {
            "metadata": {}, 
            "source": "## 2.1 Load the data\nThe first step in our analysis process is to bring our data into the Spark environment.  To do this, you will need to experiment with the 'Find and Add Data' functionality within the notebooks environment (see sidebar):\n1.  Find the entry corresponding to one of the .CSV files you uploaded as a data asset.\n2.  Click into the blank cell below this one so that you can see the cursor is ready to insert text.\n3.  Select 'insert credentials' from the drop-down menu next to the file name.\n\nIf everthing has worked correctly, you should now see some code in the cell below defining a new __Python dict__ named *credentials_1*.", 
            "cell_type": "markdown"
        }, 
        {
            "outputs": [], 
            "metadata": {
                "collapsed": false
            }, 
            "source": "# The code was removed by DSX for sharing.", 
            "execution_count": null, 
            "cell_type": "code"
        }, 
        {
            "metadata": {}, 
            "source": "__pyspark.sql__ will help us load and manipulate our data from Python while it resides in Spark.", 
            "cell_type": "markdown"
        }, 
        {
            "outputs": [], 
            "metadata": {
                "collapsed": true
            }, 
            "source": "from pyspark.sql import SparkSession\nfrom pyspark.sql import functions as F", 
            "execution_count": null, 
            "cell_type": "code"
        }, 
        {
            "metadata": {}, 
            "source": "This last statement will make some aggregation and mutation statements look a little different to the cheat sheet as spark functions will be prefixed by *F.*", 
            "cell_type": "markdown"
        }, 
        {
            "metadata": {}, 
            "source": "This function is used to setup the access of Spark to your Object Storage.", 
            "cell_type": "markdown"
        }, 
        {
            "outputs": [], 
            "metadata": {
                "collapsed": false
            }, 
            "source": "def set_hadoop_config_with_credentials(name):\n    \"\"\"This function sets the Hadoop configuration so it is possible to\n    access data from Bluemix Object Storage using Spark\"\"\"\n\n    prefix = 'fs.swift.service.' + name\n    hconf = sc._jsc.hadoopConfiguration()\n    hconf.set(prefix + '.auth.url', credentials_1[\"auth_url\"] +'/v3/auth/tokens')\n    hconf.set(prefix + '.auth.endpoint.prefix', 'endpoints')\n    hconf.set(prefix + '.tenant', credentials_1[\"project_id\"])\n    hconf.set(prefix + '.username', credentials_1[\"user_id\"])\n    hconf.set(prefix + '.password', credentials_1[\"password\"])\n    hconf.setInt(prefix + '.http.port', 8080)\n    hconf.set(prefix + '.region', credentials_1[\"region\"])\n    hconf.setBoolean(prefix + '.public', False)\n\n# you can choose any name\nname = 'keystone'\nset_hadoop_config_with_credentials(name)\n\nspark = SparkSession.builder.getOrCreate()\n", 
            "execution_count": null, 
            "cell_type": "code"
        }, 
        {
            "metadata": {}, 
            "source": "The _spark.read_ function will import a flat file into our spark instance, directly from the Object Store.", 
            "cell_type": "markdown"
        }, 
        {
            "outputs": [], 
            "metadata": {
                "collapsed": false
            }, 
            "source": "offers = spark.read\\\n  .format('org.apache.spark.sql.execution.datasources.csv.CSVFileFormat')\\\n  .option('header', 'true')\\\n  .load('swift://' + credentials_1[\"container\"] + '.' + name + '/offers.csv')\n\ntrainHistory = spark.read\\\n  .format('org.apache.spark.sql.execution.datasources.csv.CSVFileFormat')\\\n  .option('header', 'true')\\\n  .load('swift://' + credentials_1[\"container\"] + '.' + name + '/trainHistory.csv')\n    \ntransactions = spark.read\\\n  .format('org.apache.spark.sql.execution.datasources.csv.CSVFileFormat')\\\n  .option('header', 'true')\\\n  .load('swift://' + credentials_1[\"container\"] + '.' + name + '/transactions_subset.csv')\n    \noffers.cache()\ntrainHistory.cache()\ntransactions.cache()", 
            "execution_count": null, 
            "cell_type": "code"
        }, 
        {
            "metadata": {}, 
            "source": "When this is done, we are left with a Python object which is a logical pointer to a Spark DataFrame object.  \nNotice as well that Spark will give us some information on how it is breaking down the heavy lifting into discrete and parallelisable tasks.", 
            "cell_type": "markdown"
        }, 
        {
            "metadata": {}, 
            "source": "## 2.2 Quality and completeness\nBefore we can get stuck into building predictive models, we need to first understand if any immediate attention is required to issues of data quality and completeness.", 
            "cell_type": "markdown"
        }, 
        {
            "metadata": {}, 
            "source": "Like many data manipulation libraries, Spark makes its own decisions about how to read in data.  \nTo check it has treated our data as we expected, we need to compare the schemata of our Spark DataFrames to that provided with the datasets:\n\n> \n__history__  \n__id__ - A unique id representing a customer  \n__chain__ - An integer representing a store chain  \n__offer__ - An id representing a certain offer  \n__market__ - An id representing a geographical region  \n__repeattrips__ - The number of times the customer made a repeat purchase  \n__repeater__ - A boolean, equal to repeattrips > 0  \n__offerdate__ - The date a customer received the offer  \n>\n__transactions__  \n__id__ - see above  \n__chain__ - see above  \n__dept__ - An aggregate grouping of the Category (e.g. water)  \n__category__ - The product category (e.g. sparkling water)  \n__company__ - An id of the company that sells the item  \n__brand__ - An id of the brand to which the item belongs  \n__date__ - The date of purchase  \n__productsize__ - The amount of the product purchase (e.g. 16 oz of water)  \n__productmeasure__ - The units of the product purchase (e.g. ounces)  \n__purchasequantity__ - The number of units purchased  \n__purchaseamount__ - The dollar amount of the purchase  \n>\n__offers__  \n__offer__ - see above  \n__category__ - see above  \n__quantity__ - The number of units one must purchase to get the discount  \n__company__ - see above  \n__offervalue__ - The dollar value of the offer  \n__brand__ - see above  \n>\nThe transactions file can be joined to the history file by (id,chain). The history file can be joined to the offers file by (offer). The transactions file can be joined to the offers file by (category, brand, company). A negative value in productquantity and purchaseamount indicates a return.\n\nWhile we're at it, let's also see how many observations each dataset has, take a peek at the data and look for any missing values.", 
            "cell_type": "markdown"
        }, 
        {
            "metadata": {}, 
            "source": "### 2.2.1 Offer details", 
            "cell_type": "markdown"
        }, 
        {
            "metadata": {}, 
            "source": "First up, the count of observations in the dataset.", 
            "cell_type": "markdown"
        }, 
        {
            "outputs": [], 
            "metadata": {
                "collapsed": false
            }, 
            "source": "offers.count()", 
            "execution_count": null, 
            "cell_type": "code"
        }, 
        {
            "metadata": {}, 
            "source": "A small, reference dataset.", 
            "cell_type": "markdown"
        }, 
        {
            "metadata": {}, 
            "source": "Next question: how is the data typed within the Spark DataFrame?", 
            "cell_type": "markdown"
        }, 
        {
            "outputs": [], 
            "metadata": {
                "collapsed": false
            }, 
            "source": "offers.schema", 
            "execution_count": null, 
            "cell_type": "code"
        }, 
        {
            "metadata": {}, 
            "source": "Spark has interpreted every field as a string. \ud83d\ude44\n\nThere are two fields: _offervalue_ and _quantity_ that definitely should not be strings - let's fix them up now.", 
            "cell_type": "markdown"
        }, 
        {
            "outputs": [], 
            "metadata": {
                "collapsed": true
            }, 
            "source": "offers = offers.withColumn(\"offervalue\", offers[\"offervalue\"].cast(\"double\"))\noffers = offers.withColumn(\"quantity\", offers[\"quantity\"].cast(\"double\"))", 
            "execution_count": null, 
            "cell_type": "code"
        }, 
        {
            "metadata": {}, 
            "source": "Now we're ready to take a peak at the data.", 
            "cell_type": "markdown"
        }, 
        {
            "outputs": [], 
            "metadata": {
                "collapsed": false
            }, 
            "source": "offers.show()", 
            "execution_count": null, 
            "cell_type": "code"
        }, 
        {
            "metadata": {}, 
            "source": "And finally, check for any records with a missing value in critical fields.", 
            "cell_type": "markdown"
        }, 
        {
            "outputs": [], 
            "metadata": {
                "collapsed": false
            }, 
            "source": "offers.where(offers.offer.isNull() |\n            offers.category.isNull() |\n            offers.quantity.isNull() |\n            offers.company.isNull() |\n            offers.offervalue.isNull() |\n            offers.brand.isNull()).count()", 
            "execution_count": null, 
            "cell_type": "code"
        }, 
        {
            "metadata": {}, 
            "source": "### 2.2.2 Customer-Offer History", 
            "cell_type": "markdown"
        }, 
        {
            "metadata": {}, 
            "source": "Can you repeat the same operations here?\n1.  Count the records;\n2.  Inspect the type schema;\n3.  Convert miss-typed fields using _withColumn()_ and _cast()_;\n4.  Inspect the data; and\n5.  Check the dataset for missing values.", 
            "cell_type": "markdown"
        }, 
        {
            "outputs": [], 
            "metadata": {
                "collapsed": false
            }, 
            "source": "trainHistory.count()", 
            "execution_count": null, 
            "cell_type": "code"
        }, 
        {
            "metadata": {}, 
            "source": "A larger dataset that relates offers and customers.", 
            "cell_type": "markdown"
        }, 
        {
            "outputs": [], 
            "metadata": {
                "collapsed": false
            }, 
            "source": "trainHistory.schema", 
            "execution_count": null, 
            "cell_type": "code"
        }, 
        {
            "metadata": {}, 
            "source": "Same deal. Let's do some conversion to numeric and datetime types.", 
            "cell_type": "markdown"
        }, 
        {
            "outputs": [], 
            "metadata": {
                "collapsed": true
            }, 
            "source": "trainHistory = trainHistory.withColumn(\"repeattrips\", trainHistory[\"repeattrips\"].cast(\"double\"))\ntrainHistory = trainHistory.withColumn(\"repeater\", trainHistory[\"repeater\"].cast(\"boolean\"))\ntrainHistory = trainHistory.withColumn(\"repeater\", trainHistory[\"repeater\"].cast(\"double\"))\ntrainHistory = trainHistory.withColumn(\"offerdate\", trainHistory[\"offerdate\"].cast(\"date\"))", 
            "execution_count": null, 
            "cell_type": "code"
        }, 
        {
            "outputs": [], 
            "metadata": {
                "collapsed": false
            }, 
            "source": "trainHistory.show()", 
            "execution_count": null, 
            "cell_type": "code"
        }, 
        {
            "outputs": [], 
            "metadata": {
                "collapsed": false
            }, 
            "source": "trainHistory.where(trainHistory.chain.isNull() | \n                  trainHistory.market.isNull() |\n                  trainHistory.repeattrips.isNull() |\n                  trainHistory.repeater.isNull() |\n                  trainHistory.offerdate.isNull()).count()", 
            "execution_count": null, 
            "cell_type": "code"
        }, 
        {
            "metadata": {}, 
            "source": "### 2.2.3 Transactions", 
            "cell_type": "markdown"
        }, 
        {
            "outputs": [], 
            "metadata": {
                "collapsed": false
            }, 
            "source": "transactions.count()", 
            "execution_count": null, 
            "cell_type": "code"
        }, 
        {
            "metadata": {}, 
            "source": "The largest of the three datasets.", 
            "cell_type": "markdown"
        }, 
        {
            "outputs": [], 
            "metadata": {
                "collapsed": false
            }, 
            "source": "transactions.schema", 
            "execution_count": null, 
            "cell_type": "code"
        }, 
        {
            "outputs": [], 
            "metadata": {
                "collapsed": true
            }, 
            "source": "transactions = transactions.withColumn(\"date\", transactions[\"date\"].cast(\"date\"))\ntransactions = transactions.withColumn(\"productsize\", transactions[\"productsize\"].cast(\"double\"))\ntransactions = transactions.withColumn(\"purchasequantity\", transactions[\"purchasequantity\"].cast(\"double\"))\ntransactions = transactions.withColumn(\"purchaseamount\", transactions[\"purchaseamount\"].cast(\"double\"))", 
            "execution_count": null, 
            "cell_type": "code"
        }, 
        {
            "outputs": [], 
            "metadata": {
                "collapsed": false
            }, 
            "source": "transactions.show()", 
            "execution_count": null, 
            "cell_type": "code"
        }, 
        {
            "outputs": [], 
            "metadata": {
                "collapsed": false
            }, 
            "source": "transactions.where(transactions.id.isNull() |\n                  transactions.chain.isNull() |\n                  transactions.dept.isNull() |\n                  transactions.category.isNull() |\n                  transactions.company.isNull() |\n                  transactions.brand.isNull() |\n                  transactions.date.isNull() |\n                  transactions.productsize.isNull() |\n                  transactions.productmeasure.isNull() |\n                  transactions.purchasequantity.isNull() |\n                  transactions.purchaseamount.isNull()).count()", 
            "execution_count": null, 
            "cell_type": "code"
        }, 
        {
            "metadata": {}, 
            "source": "## 2.3 Exploration\nLet's begin interpreting the contents of these sets, starting with the range of dates over which the offers were presented to customers.", 
            "cell_type": "markdown"
        }, 
        {
            "outputs": [], 
            "metadata": {
                "collapsed": false
            }, 
            "source": "trainHistory.agg(\n    F.min(\"offerdate\").alias(\"offerdate_min\")\n    , F.max(\"offerdate\").alias(\"offerdate_max\")).show()", 
            "execution_count": null, 
            "cell_type": "code"
        }, 
        {
            "metadata": {}, 
            "source": "What is the frequency of records across some of the categorical variables?", 
            "cell_type": "markdown"
        }, 
        {
            "outputs": [], 
            "metadata": {
                "collapsed": false
            }, 
            "source": "trainHistory.groupBy(trainHistory.chain).count().orderBy(\"count\", ascending = False).show(n = 50)", 
            "execution_count": null, 
            "cell_type": "code"
        }, 
        {
            "metadata": {}, 
            "source": "Probably too many of these to do anything useful without grouping.", 
            "cell_type": "markdown"
        }, 
        {
            "metadata": {}, 
            "source": "Try for yourself with the _market_ variable.", 
            "cell_type": "markdown"
        }, 
        {
            "outputs": [], 
            "metadata": {
                "collapsed": false
            }, 
            "source": "trainHistory.groupBy(trainHistory.market).count().orderBy(\"count\", ascending = False).show(n = 50)", 
            "execution_count": null, 
            "cell_type": "code"
        }, 
        {
            "metadata": {}, 
            "source": "Slightly more usable, perhaps we'll come back to this.", 
            "cell_type": "markdown"
        }, 
        {
            "outputs": [], 
            "metadata": {
                "collapsed": false
            }, 
            "source": "trainHistory.describe([\"repeattrips\", \"repeater\"]).show()", 
            "execution_count": null, 
            "cell_type": "code"
        }, 
        {
            "metadata": {}, 
            "source": "First insight: 27% of customers to whom an offer is made become repeat shoppers. Does this vary across _market_?", 
            "cell_type": "markdown"
        }, 
        {
            "outputs": [], 
            "metadata": {
                "collapsed": false
            }, 
            "source": "trainHistory.groupBy(trainHistory.market).agg(\n    F.count(\"id\").alias(\"customer_count\")\n    , F.avg(\"repeater\").alias(\"response_rate\")\n    ).orderBy(\"response_rate\", ascending = False).show()", 
            "execution_count": null, 
            "cell_type": "code"
        }, 
        {
            "metadata": {}, 
            "source": "There's a hypothesis emerging here that our larger markets may show the strongest response to offers. Let's plot it to check.", 
            "cell_type": "markdown"
        }, 
        {
            "outputs": [], 
            "metadata": {
                "collapsed": false
            }, 
            "source": "count_vs_rate = trainHistory.groupBy(trainHistory.market).agg(\n    F.count(\"id\").alias(\"customer_count\")\n    , F.avg(\"repeater\").alias(\"response_rate\")\n    ).orderBy(\"response_rate\", ascending = False).toPandas()", 
            "execution_count": null, 
            "cell_type": "code"
        }, 
        {
            "outputs": [], 
            "metadata": {
                "collapsed": false
            }, 
            "source": "%matplotlib inline\nimport matplotlib.pyplot as plt\ncount_vs_rate.plot(kind='scatter', x='customer_count', y='response_rate')", 
            "execution_count": null, 
            "cell_type": "code"
        }, 
        {
            "outputs": [], 
            "metadata": {
                "collapsed": false
            }, 
            "source": "count_vs_rate[count_vs_rate.customer_count < 40000].plot(kind='scatter', x='customer_count', y='response_rate')", 
            "execution_count": null, 
            "cell_type": "code"
        }, 
        {
            "metadata": {}, 
            "source": "There is a weak relationship there, but we probably won't want to employ something as nuanced as this in our first iteration of analysis. Interesting to know though!", 
            "cell_type": "markdown"
        }, 
        {
            "metadata": {}, 
            "source": "We understand a little bit of the offer history data. Let's just check for missing values.", 
            "cell_type": "markdown"
        }, 
        {
            "metadata": {}, 
            "source": "Now we need to repeat the process across our other datasets. Let's start with the offers.", 
            "cell_type": "markdown"
        }, 
        {
            "outputs": [], 
            "metadata": {
                "collapsed": false
            }, 
            "source": "offers.describe([\"quantity\", \"offervalue\"]).show()", 
            "execution_count": null, 
            "cell_type": "code"
        }, 
        {
            "metadata": {}, 
            "source": "Looks as though a small number of the offers have a different _quantity_ value.", 
            "cell_type": "markdown"
        }, 
        {
            "outputs": [], 
            "metadata": {
                "collapsed": false
            }, 
            "source": "offers.groupBy(\"quantity\").count().show()", 
            "execution_count": null, 
            "cell_type": "code"
        }, 
        {
            "metadata": {}, 
            "source": "Interesting. Is this still applicable when we join _offers_ to our _history_ dataset?", 
            "cell_type": "markdown"
        }, 
        {
            "outputs": [], 
            "metadata": {
                "collapsed": false
            }, 
            "source": "offers[offers.quantity == 2].show()", 
            "execution_count": null, 
            "cell_type": "code"
        }, 
        {
            "outputs": [], 
            "metadata": {
                "collapsed": false
            }, 
            "source": "trainHistory[trainHistory.offer==\"1221658\"].count()", 
            "execution_count": null, 
            "cell_type": "code"
        }, 
        {
            "metadata": {}, 
            "source": "No. In which case, it's not going to be significant for our analysis and modelling.  \nDo any of the categorical fields have few enough levels to enter into a simple classification model?", 
            "cell_type": "markdown"
        }, 
        {
            "outputs": [], 
            "metadata": {
                "collapsed": false
            }, 
            "source": "offers.groupBy(\"company\").count().orderBy(\"count\", ascending = False).show()", 
            "execution_count": null, 
            "cell_type": "code"
        }, 
        {
            "outputs": [], 
            "metadata": {
                "collapsed": false
            }, 
            "source": "offers.groupBy(\"brand\").count().orderBy(\"count\", ascending = False).show()", 
            "execution_count": null, 
            "cell_type": "code"
        }, 
        {
            "outputs": [], 
            "metadata": {
                "collapsed": false
            }, 
            "source": "offers.groupBy(\"category\").count().orderBy(\"count\", ascending = False).show()", 
            "execution_count": null, 
            "cell_type": "code"
        }, 
        {
            "metadata": {}, 
            "source": "These might work in a classification tree model which automatically groups, or as a binned aggregate measure of the response rate across each.  \nLet's move on to the transactions.", 
            "cell_type": "markdown"
        }, 
        {
            "metadata": {}, 
            "source": "What is the range of dates of our transactions?", 
            "cell_type": "markdown"
        }, 
        {
            "outputs": [], 
            "metadata": {
                "collapsed": false
            }, 
            "source": "transactions.agg(\n    F.min(\"date\").alias(\"date_min\")\n    ,F.max(\"date\").alias(\"date_max\")).show()", 
            "execution_count": null, 
            "cell_type": "code"
        }, 
        {
            "metadata": {}, 
            "source": "Right, so up to a year before the offers were presented.  \nWhat are the ranges of _purchasequantity_ and _purchaseamount_?", 
            "cell_type": "markdown"
        }, 
        {
            "outputs": [], 
            "metadata": {
                "collapsed": false
            }, 
            "source": "transactions.describe([\"productsize\"\n                       ,\"productmeasure\"\n                       ,\"purchasequantity\"\n                       , \"purchaseamount\"]).show()", 
            "execution_count": null, 
            "cell_type": "code"
        }, 
        {
            "metadata": {}, 
            "source": "OK, we have some returns data in here, too. We may need to account for this in a future iteration of modelling work.   ", 
            "cell_type": "markdown"
        }, 
        {
            "metadata": {}, 
            "source": "## 2.4 Activity 2: Data understanding learning challenges", 
            "cell_type": "markdown"
        }, 
        {
            "metadata": {}, 
            "source": "### 2.4.1 Data understanding learning challenge 1\n\nIs there any pattern to the number of records in our *history* dataset by offerdate? Try plotting a graph to investigate.", 
            "cell_type": "markdown"
        }, 
        {
            "outputs": [], 
            "metadata": {
                "collapsed": false
            }, 
            "source": "datatoplot = trainHistory.groupBy(?).?\ndatatoplot.orderBy(?).show()", 
            "execution_count": null, 
            "cell_type": "code"
        }, 
        {
            "outputs": [], 
            "metadata": {
                "collapsed": false
            }, 
            "source": "datatoplot.orderBy(?).?.plot(kind=?, x=?, y=?)", 
            "execution_count": null, 
            "cell_type": "code"
        }, 
        {
            "metadata": {}, 
            "source": "The graph above is a little messy and difficult to interpret.  \nLet's use a very quick and dirty approach to extracting the month, plot that and see if this is any easier to interpret.", 
            "cell_type": "markdown"
        }, 
        {
            "outputs": [], 
            "metadata": {
                "collapsed": false
            }, 
            "source": "newdata = trainHistory.withColumn(\"offermonth\", datatoplot[\"offerdate\"][:7])\ndatatoplot = newdata.groupBy(?).? #aggregate the data\ndatatoplot.? # plot the chart", 
            "execution_count": null, 
            "cell_type": "code"
        }, 
        {
            "metadata": {}, 
            "source": "We are now able to discriminate between the periods in our analysis, but we've lost the interesting pattern we saw before.  \n\n#### Extension tasks\n1.  Try producing a summary of counts by week.  \n    Hint: take a look at the *resample()* function within **Pandas** or the **pyspark.sql.functions** functions *next_day()* or *weekofyear()*.  \n2.  Is there a pattern relating to the day of the week offers are sent out? Is this an important indicator in response likelihood?  \n    Hint: **pyspark.sql.functions**.*date_format()* may be helpful.", 
            "cell_type": "markdown"
        }, 
        {
            "outputs": [], 
            "metadata": {
                "collapsed": false
            }, 
            "source": "newdata = trainHistory.withColumn(\"offerweek\", ?)\ndatatoplot = newdata.groupBy(?).?\ndatatoplot.?.?.plot(?)", 
            "execution_count": null, 
            "cell_type": "code"
        }, 
        {
            "outputs": [], 
            "metadata": {
                "collapsed": false
            }, 
            "source": "newdata = trainHistory.withColumn(\"dayofweek\", ?) \n# hints: https://spark.apache.org/docs/1.6.1/api/python/pyspark.sql.html#pyspark.sql.functions.date_format\n# a format string of \"E\" will return a day of the week abbreviation\ndatatoplot = newdata.groupBy(?).?\ndatatoplot.?.?.plot(?)", 
            "execution_count": null, 
            "cell_type": "code"
        }, 
        {
            "metadata": {}, 
            "source": "### 2.4.2 Data understanding learning challenge 2", 
            "cell_type": "markdown"
        }, 
        {
            "metadata": {}, 
            "source": "In the data understanding phase, we run lots of operations against our data, and we need these to be optimised in order to complete the task in a timely fashion. In our case, the key differentiator is whether the function requires creation of a Python object (e.g. using Pandas), or whether it can run on a Spark data frame. Let's try using Pandas' implementation of 'describe'", 
            "cell_type": "markdown"
        }, 
        {
            "outputs": [], 
            "metadata": {
                "collapsed": false
            }, 
            "source": "trainHistory.?.?", 
            "execution_count": null, 
            "cell_type": "code"
        }, 
        {
            "outputs": [], 
            "metadata": {
                "collapsed": false
            }, 
            "source": "offers.?.?", 
            "execution_count": null, 
            "cell_type": "code"
        }, 
        {
            "metadata": {}, 
            "source": "Pyspark.sql also has an implementation of describe(), which we saw earlier. Note: be careful which order you run code, as you may need to declare which module to use a function from. Let's try to use a different set of Pyspark functionality to find the range of our continuous variables.", 
            "cell_type": "markdown"
        }, 
        {
            "outputs": [], 
            "metadata": {
                "collapsed": false
            }, 
            "source": "transactions.agg(\n    F.min(\"chain\").alias(\"chain_min\")\n    ,F.max(\"chain\").alias(\"chain_max\")\n    ,F.min(\"?\").alias(\"?\")\n    ,F.max(\"?\").alias(\"?\")\n    ?\n    ?\n    ?\n    ?\n    ?\n    ?\n).show()", 
            "execution_count": null, 
            "cell_type": "code"
        }, 
        {
            "metadata": {}, 
            "source": "From the exercise, it looks like there is something strange in one of the columns. Are all of the values positive and with a reasonable range? Let's take a look at some of the negative values", 
            "cell_type": "markdown"
        }, 
        {
            "outputs": [], 
            "metadata": {
                "collapsed": true
            }, 
            "source": "transactions[transactions.??].show()", 
            "execution_count": null, 
            "cell_type": "code"
        }, 
        {
            "metadata": {}, 
            "source": "<a id=\"prepare\"></a>\n# 3. Data preparation\nThe output of this phase is a dataset with which we can build and test models.  ", 
            "cell_type": "markdown"
        }, 
        {
            "metadata": {}, 
            "source": "## 3.1 Summarising data for use in modelling\nGiven that the aim of the task is to make customer-offer level predictions about likelihood to become a repeat purchaser, data that we use from the _offers_ or _transactions_ datasets will need to be joined to the _history_ dataset.  \nWe have also observed that the _transactions_ dataset contains a large volume of datat, too much to enter into a model without aggregation to the customer, offer level. In aggregating this, our goal is to select an approach which generates features that:\n\na)  retain as much information about the behaviour of these customers as possible; and  \nb)  will be usable in our model (some algorithms can only accept numerical inputs, for example).\n\nAs a starter set, we will simply measure how much each customer had spent in the 30, 60, 90 and 180 days prior to being made an offer.  \nTo do so, we will first need to join the offer history and transactions tables.", 
            "cell_type": "markdown"
        }, 
        {
            "outputs": [], 
            "metadata": {
                "collapsed": false
            }, 
            "source": "offertxns = transactions.join(trainHistory.select([\"id\" , \"chain\", \"offer\", \"offerdate\", \"repeater\"]), [\"id\", \"chain\"], how = \"inner\")\noffertxns.show(n=5)", 
            "execution_count": null, 
            "cell_type": "code"
        }, 
        {
            "metadata": {}, 
            "source": "Calculate \"history\" interval dates based on _offerdate_.", 
            "cell_type": "markdown"
        }, 
        {
            "outputs": [], 
            "metadata": {
                "collapsed": false
            }, 
            "source": "offertxns = offertxns.withColumn(\"offerdate_30\", F.date_sub(offertxns.offerdate, 30))\noffertxns = offertxns.withColumn(\"offerdate_60\", F.date_sub(offertxns.offerdate, 60))\noffertxns = offertxns.withColumn(\"offerdate_90\", F.date_sub(offertxns.offerdate, 90))\noffertxns = offertxns.withColumn(\"offerdate_180\", F.date_sub(offertxns.offerdate, 180))\noffertxns.show(n=5)", 
            "execution_count": null, 
            "cell_type": "code"
        }, 
        {
            "metadata": {}, 
            "source": "We can employ a Spark \"user defined function\" to create corresponding aggregation flags to identify whether the transaction in scope of one of the history periods.", 
            "cell_type": "markdown"
        }, 
        {
            "outputs": [], 
            "metadata": {
                "collapsed": false
            }, 
            "source": "from pyspark.sql.functions import udf\nfrom pyspark.sql.types import IntegerType\n\ndef inDateRange(date, date_lower, date_upper):\n    if date >= date_lower and date <= date_upper: return 1\n    else: return 0\n\nudfInDateRange = udf(inDateRange, IntegerType())\n    \noffertxns = offertxns.withColumn(\"offerdate_30_tf\", udfInDateRange(offertxns.date, offertxns.offerdate_30, offertxns.offerdate))\noffertxns = offertxns.withColumn(\"offerdate_60_tf\", udfInDateRange(offertxns.date, offertxns.offerdate_60, offertxns.offerdate))\noffertxns = offertxns.withColumn(\"offerdate_90_tf\", udfInDateRange(offertxns.date, offertxns.offerdate_90, offertxns.offerdate))\noffertxns = offertxns.withColumn(\"offerdate_180_tf\", udfInDateRange(offertxns.date, offertxns.offerdate_180, offertxns.offerdate))\noffertxns.show(n=5)", 
            "execution_count": null, 
            "cell_type": "code"
        }, 
        {
            "metadata": {}, 
            "source": "At this point we can calculate the quantity and spend per customer per offer.  \nAs an extension, you could join onto the offers table and create equivalent measures for quantity and spend in the same brand, company and category as the offer presented to the customer.", 
            "cell_type": "markdown"
        }, 
        {
            "outputs": [], 
            "metadata": {
                "collapsed": false
            }, 
            "source": "offertxns = offertxns.withColumn(\"offerdate_30_qty\", offertxns.purchasequantity * offertxns.offerdate_30_tf)\noffertxns = offertxns.withColumn(\"offerdate_60_qty\", offertxns.purchasequantity * offertxns.offerdate_60_tf)\noffertxns = offertxns.withColumn(\"offerdate_90_qty\", offertxns.purchasequantity * offertxns.offerdate_90_tf)\noffertxns = offertxns.withColumn(\"offerdate_180_qty\", offertxns.purchasequantity * offertxns.offerdate_180_tf)\n\noffertxns = offertxns.withColumn(\"offerdate_30_amt\", offertxns.purchaseamount * offertxns.offerdate_30_tf)\noffertxns = offertxns.withColumn(\"offerdate_60_amt\", offertxns.purchaseamount * offertxns.offerdate_60_tf)\noffertxns = offertxns.withColumn(\"offerdate_90_amt\", offertxns.purchaseamount * offertxns.offerdate_90_tf)\noffertxns = offertxns.withColumn(\"offerdate_180_amt\", offertxns.purchaseamount * offertxns.offerdate_180_tf)\noffertxns.show(n=5)", 
            "execution_count": null, 
            "cell_type": "code"
        }, 
        {
            "outputs": [], 
            "metadata": {
                "collapsed": false
            }, 
            "source": "offertxnsSum = offertxns.groupBy([\"id\", \"chain\", \"offer\", \"offerdate\", \"repeater\"]).agg(\n    F.sum(\"offerdate_30_qty\").alias(\"qty_30\")\n    , F.sum(\"offerdate_60_qty\").alias(\"qty_60\")\n    , F.sum(\"offerdate_90_qty\").alias(\"qty_90\")\n    , F.sum(\"offerdate_180_qty\").alias(\"qty_180\")\n    , F.sum(\"offerdate_30_amt\").alias(\"amt_30\")\n    , F.sum(\"offerdate_60_amt\").alias(\"amt_60\")\n    , F.sum(\"offerdate_90_amt\").alias(\"amt_90\")\n    , F.sum(\"offerdate_180_amt\").alias(\"amt_180\"))\noffertxnsSum.show(n=5)", 
            "execution_count": null, 
            "cell_type": "code"
        }, 
        {
            "metadata": {}, 
            "source": "## 3.2 Reshaping data", 
            "cell_type": "markdown"
        }, 
        {
            "metadata": {}, 
            "source": "What is the average spend in these intervals?  \nSpark will allow us to calculate this quite easily. In order to plot this nicely, we will need help from the Python data wrangling library of choice: Pandas. Luckily Spark also offers an easy way to translate between the two types of object using the  _.toPandas()_ function.", 
            "cell_type": "markdown"
        }, 
        {
            "outputs": [], 
            "metadata": {
                "collapsed": false
            }, 
            "source": "import pandas as pd\naverage_spend = offertxnsSum.groupBy(\"repeater\").agg(\n    F.avg(\"amt_30\").alias(\"30\")\n    , F.avg(\"amt_60\").alias(\"60\")\n    , F.avg(\"amt_90\").alias(\"90\")\n    , F.avg(\"amt_180\").alias(\"180\")).toPandas()\n\naverage_spend_melt = pd.melt(average_spend, id_vars = \"repeater\", var_name = \"interval_days\", value_name = \"spend_ave\")\naverage_spend_melt[\"interval_days\"] = pd.to_numeric(average_spend_melt[\"interval_days\"])\n\naverage_spend_melt.head()", 
            "execution_count": null, 
            "cell_type": "code"
        }, 
        {
            "outputs": [], 
            "metadata": {
                "collapsed": false
            }, 
            "source": "average_spend_melt.plot(kind='scatter', x='interval_days', y='spend_ave', c=\"repeater\")", 
            "execution_count": null, 
            "cell_type": "code"
        }, 
        {
            "metadata": {}, 
            "source": "## 3.3 Divertissement: interactive charting\n*(Maybe useful for the hack later...)*  \n\nIf we want to add some interactivity to our charts, one great option is the Bokeh library.", 
            "cell_type": "markdown"
        }, 
        {
            "outputs": [], 
            "metadata": {
                "collapsed": false
            }, 
            "source": "from bokeh.plotting import figure\nfrom bokeh.io import output_notebook, show\nfrom bokeh.charts import Scatter\nfrom bokeh.palettes import brewer\n\noutput_notebook()", 
            "execution_count": null, 
            "cell_type": "code"
        }, 
        {
            "outputs": [], 
            "metadata": {
                "collapsed": false
            }, 
            "source": "p = Scatter(average_spend_melt,\n              x=\"interval_days\",\n              y=\"spend_ave\",\n            color = \"repeater\",\n              title = \"Comparison: purchase intervals and average spends\",\n             xlabel = \"Purchase analysis interval\",\n             ylabel = \"Average spend\",\n           palette = brewer[\"Dark2\"][3])\nshow(p)", 
            "execution_count": null, 
            "cell_type": "code"
        }, 
        {
            "metadata": {}, 
            "source": "This gives us a very small level of interaction - but much more is possible!  \nYou may have to move outside of the notebook environment to do this though (using output_file, for example).  \n[Bokeh interactivity docs and examples](http://bokeh.pydata.org/en/0.11.1/docs/user_guide/interaction.html)", 
            "cell_type": "markdown"
        }, 
        {
            "metadata": {}, 
            "source": "## 3.4 Activity 3: Data prep learning challenges", 
            "cell_type": "markdown"
        }, 
        {
            "metadata": {}, 
            "source": "### 3.4.1 Data prep learning challenge 1\nLet's apply some of what we've looked at together and engineer a new feature that may or may not be predictive and useful, but will at least give us some experience of working with the summarisation capability of _pyspark.sql_.  \n\nWhat we are looking to do is examine which departments customers have shopped into as a way of classifying their habits.", 
            "cell_type": "markdown"
        }, 
        {
            "metadata": {}, 
            "source": "Let's start by identifying the five most popular departments in our transactions set. We can sample our data to achieve this quickly.", 
            "cell_type": "markdown"
        }, 
        {
            "outputs": [], 
            "metadata": {
                "collapsed": false
            }, 
            "source": "(transactions\n     .sample(False, 0.01, 42)\n     .groupBy(?)\n     .agg(?.alias(\"transaction_count\"))\n     .orderBy(?)\n     .show(?))", 
            "execution_count": null, 
            "cell_type": "code"
        }, 
        {
            "metadata": {}, 
            "source": "We can use these to create a series of flags at the customer level which may be useful in the classification task.", 
            "cell_type": "markdown"
        }, 
        {
            "outputs": [], 
            "metadata": {
                "collapsed": false
            }, 
            "source": "customerDepts = (transactions\n                 .withColumn(?, (?).?) # check out the cheat sheet for help here!\n                    ...\n                 .groupBy(?)\n                 .agg(?.alias(\"dept_99\"),\n                     ...)) # how would you aggregate this to get a 'per customer' answer?", 
            "execution_count": null, 
            "cell_type": "code"
        }, 
        {
            "metadata": {}, 
            "source": "Inspect and generate some summary statistics for these fields.", 
            "cell_type": "markdown"
        }, 
        {
            "outputs": [], 
            "metadata": {
                "collapsed": false
            }, 
            "source": "customerDepts.?", 
            "execution_count": null, 
            "cell_type": "code"
        }, 
        {
            "outputs": [], 
            "metadata": {
                "collapsed": false
            }, 
            "source": "customerDepts.?.?", 
            "execution_count": null, 
            "cell_type": "code"
        }, 
        {
            "metadata": {}, 
            "source": "Last piece of the puzzle: let's measure the level of correlation of these predictors.  \n(If everybody buys from the same departments then these will not be good predictor variables to use in a model.)\n\n**Pandas** has a very slick way of doing this via. the *.corr()* function, but what do we need to do first to allow us to use our summary data?", 
            "cell_type": "markdown"
        }, 
        {
            "outputs": [], 
            "metadata": {
                "collapsed": false
            }, 
            "source": "customerDepts.?.corr()", 
            "execution_count": null, 
            "cell_type": "code"
        }, 
        {
            "metadata": {}, 
            "source": "#### Qs.\n1.  Are these good variables to add into the model?\n2.  How else could we test their suitability?\n3.  If we built these flags for a hundred departments: how could we use this information but still only add a small number of additional predictors into our model?", 
            "cell_type": "markdown"
        }, 
        {
            "metadata": {}, 
            "source": "### 3.4.2 Data prep learning challenge 2\nHow could we add a new column to our offer history data, expressing the rank of the chain (by number of customers) within the market?", 
            "cell_type": "markdown"
        }, 
        {
            "metadata": {}, 
            "source": "#### Wrangling pattern:\n1.  Summarise customer counts by chain and market;\n2.  Apply the *rank()* function to customer counts by market; then\n3.  Join back to original dataframe.", 
            "cell_type": "markdown"
        }, 
        {
            "metadata": {}, 
            "source": "Starting with (1) - can you use the _agg()_ function to summarise the data appropriately?  \n-  Name your column of counts: *customer_count*\n-  Name your new dataframe: *historyCustCounts*", 
            "cell_type": "markdown"
        }, 
        {
            "outputs": [], 
            "metadata": {
                "collapsed": false
            }, 
            "source": "historyCustCount = (trainHistory\n                    .groupBy(?,?)\n                    .agg(?))\nhistoryCustCount.show()\nhistoryCustCount.count()", 
            "execution_count": null, 
            "cell_type": "code"
        }, 
        {
            "metadata": {}, 
            "source": "Moving on to (2), let's calculate a new column *chain_market_rank* within _historyCustCount_ showing the ranking of chains by customer count within each market.  \n**Hint**: as this is a window function (calculates a quantity within a partition, in this case _market_), you'll need to specify the window specification using _Window_ which is available in the _pyspark.sql.window_ library.", 
            "cell_type": "markdown"
        }, 
        {
            "outputs": [], 
            "metadata": {
                "collapsed": false
            }, 
            "source": "from ? import ?\nw = ?.partitionBy(?).orderBy(?)\nhistoryCustCount = historyCustCount.withColumn(?, ?.over(w))\nhistoryCustCount.show()", 
            "execution_count": null, 
            "cell_type": "code"
        }, 
        {
            "metadata": {}, 
            "source": "Is this a good variable to use in our model? Let's plot the distribution of values as a histogram.\n\nAgain, Pandas has a very neat function: *.hist()* that allows us to plot histograms quickly.", 
            "cell_type": "markdown"
        }, 
        {
            "outputs": [], 
            "metadata": {
                "collapsed": false
            }, 
            "source": "historyCustCount.?.hist(?) # check out the pandas docs for help with the arguments to hist()\n# https://pandas.pydata.org/pandas-docs/stable/visualization.html#visualization-hist", 
            "execution_count": null, 
            "cell_type": "code"
        }, 
        {
            "metadata": {}, 
            "source": "Do you think this would be informative?  \nWhat other analysis could you do to support your assertion?", 
            "cell_type": "markdown"
        }, 
        {
            "metadata": {}, 
            "source": "For completeness, let's go ahead and join this back to our original _trainHistory_ dataframe.", 
            "cell_type": "markdown"
        }, 
        {
            "outputs": [], 
            "metadata": {
                "collapsed": false
            }, 
            "source": "trainHistory = ?.join(?) # pyspark cheat sheet will help you here\ntrainHistory.show()", 
            "execution_count": null, 
            "cell_type": "code"
        }, 
        {
            "metadata": {}, 
            "source": "We don't need *customer_count*, let's go ahead and drop it.", 
            "cell_type": "markdown"
        }, 
        {
            "outputs": [], 
            "metadata": {
                "collapsed": false
            }, 
            "source": "trainHistory = trainHistory.?\ntrainHistory.show()", 
            "execution_count": null, 
            "cell_type": "code"
        }, 
        {
            "metadata": {}, 
            "source": "### 3.4.3 Data prep learning challenge 3\nThis challenge is more open ended: you will work from a specification to produce a new predictor variable (i.e. without direction from the script).", 
            "cell_type": "markdown"
        }, 
        {
            "metadata": {}, 
            "source": "This is the specification you have been provided:\n\n#### Calculating \"Customer first transaction interval\"\nWe are hoping to create a variable to encapsulate information about how long a customer began shopping with the company prior to being offered an incentive.  \nTo do this, you will need to:\n1.  Find the first transaction for each customer in the transactions dataset;\n2.  Compare that to the _offerdate_ in the offer history dataset and calculate the number of days between the two (the *datediff()* function will help); and\n3.  Plot (or otherwise analyse) the distribution and make a decision about whether you would include this in a model. You might consider sampling your data before plotting.", 
            "cell_type": "markdown"
        }, 
        {
            "outputs": [], 
            "metadata": {
                "collapsed": false
            }, 
            "source": "firstCustTrans = (transactions\n                  .groupBy(?)\n                  .agg(?.alias(\"first_purch_date\")))\nfirstCustTrans.show()", 
            "execution_count": null, 
            "cell_type": "code"
        }, 
        {
            "outputs": [], 
            "metadata": {
                "collapsed": false
            }, 
            "source": "firstCustTrans = (trainHistory\n                  .select(\"id\", \"offerdate\")\n                  .join(firstCustTrans, ?)\n                  .withColumn(\"shop_history_interval\", ?))\n\nfirstCustTrans.show()", 
            "execution_count": null, 
            "cell_type": "code"
        }, 
        {
            "outputs": [], 
            "metadata": {
                "collapsed": false
            }, 
            "source": "firstCustTrans.sample(?).?.?", 
            "execution_count": null, 
            "cell_type": "code"
        }, 
        {
            "metadata": {}, 
            "source": "<a id=\"model\"></a>\n# 4. Modelling experiments\nWe're now ready to have a first pass at building a model.  ", 
            "cell_type": "markdown"
        }, 
        {
            "metadata": {}, 
            "source": "## 4.1 Holdout partitioning\nFor those who have worked in targeted marketing, this approach will be quite familiar. The premise is to train a model based on one part of your dataset and evaluate its performance using the other.  \nSpark has a convenience function to do just that: _randomSplit()_. The arguments supplied to this function is an array of weights specifying the proportions by which the dataset should be split.", 
            "cell_type": "markdown"
        }, 
        {
            "outputs": [], 
            "metadata": {
                "collapsed": false
            }, 
            "source": "offertxnsSum = offertxnsSum.withColumnRenamed(\"repeater\",\"label\")\nsplits = offertxnsSum.randomSplit([0.7,0.3])\ntrainSet = splits[0]\ntestSet = splits[1]\n\ntrainSet.show(n=5)", 
            "execution_count": null, 
            "cell_type": "code"
        }, 
        {
            "outputs": [], 
            "metadata": {
                "collapsed": false
            }, 
            "source": "trainSet.count()", 
            "execution_count": null, 
            "cell_type": "code"
        }, 
        {
            "metadata": {}, 
            "source": "Let's also cache these datafrmaes like we did with our original data sets.", 
            "cell_type": "markdown"
        }, 
        {
            "outputs": [], 
            "metadata": {
                "collapsed": false
            }, 
            "source": "trainSet.cache()\ntestSet.cache()", 
            "execution_count": null, 
            "cell_type": "code"
        }, 
        {
            "metadata": {}, 
            "source": "Spark has an idiosyncratic way of taking input and we will have to employ the 'VectorAssembler' function to bundle up the features for us to input into the model.", 
            "cell_type": "markdown"
        }, 
        {
            "outputs": [], 
            "metadata": {
                "collapsed": false
            }, 
            "source": "from pyspark.ml.feature import VectorAssembler\n\nassembler = VectorAssembler(\n    inputCols=[\"qty_30\", \"qty_60\", \"qty_90\", \"qty_180\",\n              \"amt_30\", \"amt_60\", \"amt_90\", \"amt_180\"],\n    outputCol=\"features\")\n\ntrainSetAssembled = assembler.transform(trainSet)\ntrainSetAssembled.show(n=5, truncate = False)", 
            "execution_count": null, 
            "cell_type": "code"
        }, 
        {
            "outputs": [], 
            "metadata": {
                "collapsed": false
            }, 
            "source": "from pyspark.ml import Pipeline\nfrom pyspark.ml.classification import LogisticRegression\nlr = LogisticRegression()\npipeline = Pipeline(stages=[assembler, lr])\nmodel = pipeline.fit(trainSet)", 
            "execution_count": null, 
            "cell_type": "code"
        }, 
        {
            "metadata": {}, 
            "source": "We now have a fitted model!  \nWhat values have been chosen for the parameters?", 
            "cell_type": "markdown"
        }, 
        {
            "outputs": [], 
            "metadata": {
                "collapsed": false
            }, 
            "source": "#[stage.coefficients for stage in model.stages if hasattr(stage, \"coefficients\")]\nmodel.stages[1].coefficients", 
            "execution_count": null, 
            "cell_type": "code"
        }, 
        {
            "outputs": [], 
            "metadata": {
                "collapsed": false
            }, 
            "source": "prediction = model.transform(trainSet)\nprediction.select(\"label\",\"prediction\", \"probability\", \"features\").show(5)", 
            "execution_count": null, 
            "cell_type": "code"
        }, 
        {
            "outputs": [], 
            "metadata": {
                "collapsed": true
            }, 
            "source": "from sklearn.metrics import roc_curve\n\nprediction_collect = prediction.select(\"label\", \"probability\").toPandas()\nroc_inputs = [(float(i[1][0]), float(i[1][1][1])) for i in prediction_collect.iterrows()]\nroc_inputs = pd.DataFrame(roc_inputs, columns = [\"label\",\"prob\"])\nfpr, tpr, _ = roc_curve(y_true = roc_inputs.label, \n                        y_score = roc_inputs.prob)\n\nroc_lr_train = pd.DataFrame(\n   {\"FPR\" : fpr\n    ,\"TPR\" : tpr})", 
            "execution_count": null, 
            "cell_type": "code"
        }, 
        {
            "outputs": [], 
            "metadata": {
                "collapsed": false
            }, 
            "source": "fig = plt.figure()\nax = plt.axes()\nax.set(title = \"Receiver-Operator Characteristic\",\n       xlabel = \"False Positive Rate\",\n      ylabel = \"True Positive Rate\")\n\nx = [0,1]\ny = [0,1]\nax.plot(x, y)\nax.plot(roc_lr_train.FPR, roc_lr_train.TPR)\nplt.show()", 
            "execution_count": null, 
            "cell_type": "code"
        }, 
        {
            "outputs": [], 
            "metadata": {
                "collapsed": false
            }, 
            "source": "from pyspark.ml.evaluation import BinaryClassificationEvaluator\n\nevaluator = BinaryClassificationEvaluator(\n    labelCol=\"label\", rawPredictionCol=\"rawPrediction\", metricName=\"areaUnderROC\")\naccuracy = evaluator.evaluate(prediction)\nprint \"Area under the ROC curve = %g \" % accuracy", 
            "execution_count": null, 
            "cell_type": "code"
        }, 
        {
            "metadata": {}, 
            "source": "Not bad, the competition benchmark for this dataset is 0.59.  \nBefore we start extending this, we should perform some diagnositcs.\n\nFirst, let's look at where the misclassifications are occuring by creating a contingency table (a.k.a confusion matrix).", 
            "cell_type": "markdown"
        }, 
        {
            "outputs": [], 
            "metadata": {
                "collapsed": true
            }, 
            "source": "prediction_collect = prediction.toPandas()\npd.crosstab(prediction_collect.label, prediction_collect.prediction)", 
            "execution_count": null, 
            "cell_type": "code"
        }, 
        {
            "metadata": {}, 
            "source": "What does this tell us? Is this a bad model?", 
            "cell_type": "markdown"
        }, 
        {
            "metadata": {}, 
            "source": "And of course we should check if our model generalises well by scoring and evaluating the test set.  ", 
            "cell_type": "markdown"
        }, 
        {
            "metadata": {}, 
            "source": "## 4.2 Modeling Exercises\n### 4.2.1 Exercise 1: Scoring on the test set", 
            "cell_type": "markdown"
        }, 
        {
            "metadata": {}, 
            "source": "Go ahead and use the pipeline we've already developed to transform the test set data. \nWhen you've done that, plot a ROC curve for the predictions and measure the AUC. \nDiscuss and draw some conclusions about whether the model is generalising well.", 
            "cell_type": "markdown"
        }, 
        {
            "outputs": [], 
            "metadata": {
                "collapsed": false
            }, 
            "source": "prediction = ?\nprediction.select(\"label\",\"prediction\", \"probability\", \"features\").show(5)", 
            "execution_count": null, 
            "cell_type": "code"
        }, 
        {
            "metadata": {}, 
            "source": "Build a confusion matrix to calculate rate of misclassification", 
            "cell_type": "markdown"
        }, 
        {
            "outputs": [], 
            "metadata": {
                "collapsed": false
            }, 
            "source": "prediction_collect = prediction.?\npd.crosstab(?)", 
            "execution_count": null, 
            "cell_type": "code"
        }, 
        {
            "outputs": [], 
            "metadata": {
                "collapsed": true
            }, 
            "source": "prediction_collect = prediction.? # different to the previous cell!\nroc_inputs = ?\nroc_lr_test = ?", 
            "execution_count": null, 
            "cell_type": "code"
        }, 
        {
            "outputs": [], 
            "metadata": {
                "collapsed": false
            }, 
            "source": "fig = plt.figure()\nax = plt.axes()\nax.set(title = \"Receiver-Operator Characteristic\",\n       xlabel = \"False Positive Rate\",\n      ylabel = \"True Positive Rate\")\n\nx = [0,1]\ny = [0,1]\nax.plot(x, y)\nax.plot(roc_lr_train.FPR, roc_lr_train.TPR)\nax.plot(?) # this is where you would plot your test set performance\n# specify a different line colour or style to differentiate from the training set performance.\nplt.show()", 
            "execution_count": null, 
            "cell_type": "code"
        }, 
        {
            "outputs": [], 
            "metadata": {
                "collapsed": false
            }, 
            "source": "accuracy = ?\nprint \"Area under the ROC curve = %g \" % accuracy", 
            "execution_count": null, 
            "cell_type": "code"
        }, 
        {
            "metadata": {}, 
            "source": "### 4.2.2 Exercise 2: Creating a Decision Tree", 
            "cell_type": "markdown"
        }, 
        {
            "metadata": {}, 
            "source": "We've seen how to create a logistic regression model. However, this is a parametric model and requires making some distributional assumptions about our data. In some cases this is not appropriate and we need to use a non-parametric method. Let's go through the same approach using pyspark.ml.classification, and fit a decision tree.  \nSlight additional complexity here: regardless of the original data type of the target variable, this algorithm requires you to have processed it with StringIndexer (from pyspark.ml.feature) prior to sending it to the model. Hence we have an additional stage to this pipeline.", 
            "cell_type": "markdown"
        }, 
        {
            "outputs": [], 
            "metadata": {
                "collapsed": true
            }, 
            "source": "from pyspark.ml.classification import ?\nfrom pyspark.ml.feature import StringIndexer\nsi = StringIndexer(inputCol=\"label\", outputCol=\"indexed\")\ndt = DecisionTreeClassifier(labelCol=?)\npipeline_dt = Pipeline(stages=[?, ?, ?])\nmodel_dt = ?", 
            "execution_count": null, 
            "cell_type": "code"
        }, 
        {
            "metadata": {}, 
            "source": "The first thing we did with our logistic regression was to look at the parameter values. There is no equivalent for decision trees. Instead, there is a featureImportances object which will give us similar useful information about the model. \nExtract it from the pipeline in the same way we did for the coefficients of our logistic regression.", 
            "cell_type": "markdown"
        }, 
        {
            "outputs": [], 
            "metadata": {
                "collapsed": false
            }, 
            "source": "model_dt.stages[?].?", 
            "execution_count": null, 
            "cell_type": "code"
        }, 
        {
            "metadata": {}, 
            "source": "Go ahead and measure the AUC metric as before using the trains and test sets.", 
            "cell_type": "markdown"
        }, 
        {
            "outputs": [], 
            "metadata": {
                "collapsed": false
            }, 
            "source": "prediction_dt = ?\nprediction_dt.select(\"label\", \"prediction\", \"probability\").show(5)", 
            "execution_count": null, 
            "cell_type": "code"
        }, 
        {
            "outputs": [], 
            "metadata": {
                "collapsed": false
            }, 
            "source": "evaluator_dt = BinaryClassificationEvaluator(?)\naccuracy_dt = ?\nprint \"Area under the ROC curve = %g \" % accuracy_dt", 
            "execution_count": null, 
            "cell_type": "code"
        }, 
        {
            "outputs": [], 
            "metadata": {
                "collapsed": false
            }, 
            "source": "prediction_collect = ?\npd.crosstab(?)", 
            "execution_count": null, 
            "cell_type": "code"
        }, 
        {
            "metadata": {}, 
            "source": "Now, there is a stochastic element to the building of these models, but in preparation for Cognihack, something felt strange about the AUC we were getting. Have a chat with your teams about why this may be the case, in particular in the context of parallel computing.", 
            "cell_type": "markdown"
        }, 
        {
            "metadata": {}, 
            "source": "#### Extension: ", 
            "cell_type": "markdown"
        }, 
        {
            "metadata": {}, 
            "source": "- Try recreating the decision tree model, carefully selecting your features. \n- What about any more data derivations you could include\n- Try selecting the parameters of the decision tree, such as depth and minimum split size\n- Consider other classification algorithms, either from pyspark.ml or elsewhere", 
            "cell_type": "markdown"
        }, 
        {
            "metadata": {}, 
            "source": "<a id=\"deploy\"></a>\n# 5. Deploy and score\n\nWith the advent of __Watson Machine Learning__, we can quite easily deploy our model to a cloud scoring service.  ", 
            "cell_type": "markdown"
        }, 
        {
            "metadata": {}, 
            "source": "## 5.1 Persist Spark model within the ML repository\n\nThe first step here is to import the relevant client libraries.", 
            "cell_type": "markdown"
        }, 
        {
            "outputs": [], 
            "metadata": {
                "collapsed": true
            }, 
            "source": "from repository.mlrepositoryclient import MLRepositoryClient\nfrom repository.mlrepositoryartifact import MLRepositoryArtifact", 
            "execution_count": null, 
            "cell_type": "code"
        }, 
        {
            "metadata": {}, 
            "source": "**Tip**: service_path, user and password can be found on **Service Credentials** tab of service instance created in Bluemix.", 
            "cell_type": "markdown"
        }, 
        {
            "metadata": {}, 
            "source": "For example, the following code:  \n```\nwml_service_path = \"https://ibm-watson-ml.mybluemix.net\"  \nwml_username = \"ebecda6c-a18b-4c6f-82e4-4c7fc26361f4\"  \nwml_password = \"4705d497-fcc0-4e1c-9f55-934b13b13fb2\"  \n```\nWill create the necessary credentials to connect to the Watson ML service. Just substitute in your own in the place of these example values.", 
            "cell_type": "markdown"
        }, 
        {
            "outputs": [], 
            "metadata": {
                "collapsed": true
            }, 
            "source": "# The code was removed by DSX for sharing.", 
            "execution_count": null, 
            "cell_type": "code"
        }, 
        {
            "outputs": [], 
            "metadata": {
                "collapsed": false
            }, 
            "source": "ml_repository_client = MLRepositoryClient(wml_service_path)\nml_repository_client.authorize(wml_username, wml_password)", 
            "execution_count": null, 
            "cell_type": "code"
        }, 
        {
            "metadata": {}, 
            "source": "Create model artifact (abstraction layer).", 
            "cell_type": "markdown"
        }, 
        {
            "outputs": [], 
            "metadata": {
                "collapsed": true
            }, 
            "source": "model_artifact = MLRepositoryArtifact(model, training_data=trainSet, name=\"Repeat Buyer Prediction Model\")", 
            "execution_count": null, 
            "cell_type": "code"
        }, 
        {
            "metadata": {}, 
            "source": "**Tip**: The MLRepositoryArtifact method expects a trained model object, training data, and a model name. (It is this model name that is displayed by the Watson Machine Learning service).  ", 
            "cell_type": "markdown"
        }, 
        {
            "metadata": {}, 
            "source": "We can now save our model to the repository.", 
            "cell_type": "markdown"
        }, 
        {
            "outputs": [], 
            "metadata": {
                "collapsed": true
            }, 
            "source": "saved_model = ml_repository_client.models.save(model_artifact)", 
            "execution_count": null, 
            "cell_type": "code"
        }, 
        {
            "metadata": {}, 
            "source": "Get saved model metadata from Watson Machine Learning.  \n**Tip**: Use *meta.available_props()* to get the list of available props.", 
            "cell_type": "markdown"
        }, 
        {
            "outputs": [], 
            "metadata": {
                "collapsed": false
            }, 
            "source": "saved_model.meta.available_props()", 
            "execution_count": null, 
            "cell_type": "code"
        }, 
        {
            "outputs": [], 
            "metadata": {
                "collapsed": false
            }, 
            "source": "print \"modelType: \" + saved_model.meta.prop(\"modelType\")\nprint \"trainingDataSchema: \" + str(saved_model.meta.prop(\"trainingDataSchema\"))\nprint \"creationTime: \" + str(saved_model.meta.prop(\"creationTime\"))\nprint \"modelVersionHref: \" + saved_model.meta.prop(\"modelVersionHref\")\nprint \"label: \" + saved_model.meta.prop(\"label\")", 
            "execution_count": null, 
            "cell_type": "code"
        }, 
        {
            "metadata": {}, 
            "source": "**Tip**: **modelVersionHref** is our model unique indentifier in the Watson Machine Learning repository.", 
            "cell_type": "markdown"
        }, 
        {
            "metadata": {}, 
            "source": "## 5.2 Create an online scoring endpoint\nIn this section you will learn how to create online scoring and to score a new data record by using the Watson Machine Learning REST API.  \nFor more information about REST APIs, see the [Swagger Documentation](http://watson-ml-api.mybluemix.net/).  \nTo work with the Watson Machine Leraning REST API you must generate an access token. To do that you can use the following sample code:", 
            "cell_type": "markdown"
        }, 
        {
            "outputs": [], 
            "metadata": {
                "collapsed": false
            }, 
            "source": "import urllib3, requests, json\n\nheaders = urllib3.util.make_headers(basic_auth='{}:{}'.format(wml_username, wml_password))\nurl = '{}/v2/identity/token'.format(wml_service_path)\nresponse = requests.get(url, headers=headers)\nmltoken = json.loads(response.text).get('token')\nprint mltoken", 
            "execution_count": null, 
            "cell_type": "code"
        }, 
        {
            "metadata": {}, 
            "source": "You can now create an online scoring endpoint. Execute the following sample code that uses the **modelVersionHref** value to create the scoring endpoint to the Bluemix repository.", 
            "cell_type": "markdown"
        }, 
        {
            "outputs": [], 
            "metadata": {
                "collapsed": false
            }, 
            "source": "endpoint_online = wml_service_path + \"/v2/online/deployments/\"\nheader_online = {'Content-Type': 'application/json', 'Authorization': mltoken}\npayload_online = {\"artifactVersionHref\": saved_model.meta.prop(\"modelVersionHref\"), \"name\": \"Repeat Shopper Prediction\"}\n\nprint endpoint_online\nprint header_online\nprint payload_online\n\nresponse_online = requests.post(endpoint_online, json=payload_online, headers=header_online)\n\nprint response_online\nprint response_online.text\n\nscoring_href = json.loads(response_online.text).get('entity').get('scoringHref')\nprint scoring_href", 
            "execution_count": null, 
            "cell_type": "code"
        }, 
        {
            "metadata": {
                "collapsed": false
            }, 
            "source": "Let's see what happens when we send a PUT request to our new endpoint containing a new scoring record. The model should hopefully return some predictions.", 
            "cell_type": "markdown"
        }, 
        {
            "outputs": [], 
            "metadata": {
                "collapsed": false
            }, 
            "source": "payload_scoring = {\n    \"record\":[\n        \"42\", #id                     \n        \"8620\", #chain\n        \"400\", #offer\n        \"2017-6-5\", #offerdate\n        5, #qty_30\n        10, #qty_60\n        15, #qty_90\n        20, #qty_180\n        50, #amt_30\n        100, #amt_60\n        150, #amt_90\n        200, #amt_180\n    ]}\n\nresponse_scoring = requests.put(scoring_href, json=payload_scoring, headers=header_online)\n\nprint response_scoring.text", 
            "execution_count": null, 
            "cell_type": "code"
        }, 
        {
            "outputs": [], 
            "metadata": {
                "collapsed": true
            }, 
            "source": "", 
            "execution_count": null, 
            "cell_type": "code"
        }
    ]
}